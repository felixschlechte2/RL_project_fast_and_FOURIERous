- QR-SAC vs random agent
- alle rewards sind angestellt: closenes to puck, touch puck, puck direction
- lasse es f√ºr 5 h laufen / ~ 60000 Epochen / ~ 10.000.000 steps 
- alle pars.args auf Default (--cuda und -- num_steps = 10000001)
- + layer normalization
- reward * 100
- Agent hat 4 outputs und im Buffer wird nur Aktion vom agent gespeichert 
- mit 16 quantilen und traj length 7 