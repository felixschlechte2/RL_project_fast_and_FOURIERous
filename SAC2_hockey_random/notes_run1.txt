- SAC vs random agent
- alle rewards sind angestellt: closenes to puck, touch puck, puck direction
- lasse es f√ºr 9 h laufen / ~ 8400 Epochen / ~ 1.100.000 steps 
- alle pars.args auf Default (--cude und -- num_steps = 3000001)
- + layer normalization
- reward * 100
- Agent hat 4 outputs und im Buffer wird nur Aktion vom agent gespeichert 